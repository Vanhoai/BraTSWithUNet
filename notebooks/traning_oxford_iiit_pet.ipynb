{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1963c672",
   "metadata": {},
   "source": [
    "##### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c10889c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
      "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.12/dist-packages (1.8.2)\n",
      "Requirement already satisfied: albumentations in /usr/local/lib/python3.12/dist-packages (2.0.8)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (0.15.2)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from albumentations) (1.16.3)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from albumentations) (6.0.3)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.12/dist-packages (from albumentations) (2.11.10)\n",
      "Requirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.12/dist-packages (from albumentations) (0.0.24)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.12/dist-packages (from albumentations) (4.12.0.88)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.12/dist-packages (from albucore==0.0.24->albumentations) (4.2.3)\n",
      "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.12/dist-packages (from albucore==0.0.24->albumentations) (6.5.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python torch torchvision torchmetrics albumentations matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1ef28251b463bd",
   "metadata": {},
   "source": [
    "##### Required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d67c6daaf4727e59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T13:33:36.079997Z",
     "start_time": "2025-11-26T13:33:33.764706Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from typing import List, Tuple\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2  # np.array -> torch.tensor\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchmetrics.segmentation import GeneralizedDiceScore, MeanIoU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83493aa0b347a9b0",
   "metadata": {},
   "source": [
    "##### Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd240421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-972715178.py:15: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar.extractall(\"/content/data\")\n",
      "/tmp/ipython-input-972715178.py:18: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar.extractall(\"/content/data\")\n"
     ]
    }
   ],
   "source": [
    "# Download and prepare the dataset\n",
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "os.makedirs(\"/content/data\", exist_ok=True)\n",
    "\n",
    "# Download Oxford-IIIT Pet Dataset\n",
    "url = \"https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\"\n",
    "urllib.request.urlretrieve(url, \"/content/data/images.tar.gz\")\n",
    "\n",
    "url_annotations = \"https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\"\n",
    "urllib.request.urlretrieve(url_annotations, \"/content/data/annotations.tar.gz\")\n",
    "\n",
    "with tarfile.open(\"/content/data/images.tar.gz\", \"r:gz\") as tar:\n",
    "    tar.extractall(\"/content/data\")\n",
    "    \n",
    "with tarfile.open(\"/content/data/annotations.tar.gz\", \"r:gz\") as tar:\n",
    "    tar.extractall(\"/content/data\")\n",
    "\n",
    "root = \"/content/data\"\n",
    "saved_directory = \"/content/saved\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41caaf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotations  annotations.tar.gz  images  images.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!ls /content/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65796e3362bc30db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T13:34:03.370618Z",
     "start_time": "2025-11-26T13:34:03.365139Z"
    }
   },
   "outputs": [],
   "source": [
    "class OxfordIIIPetDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        is_train: bool = True,\n",
    "        transform=None,\n",
    "    ):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.classes = [\"background\", \"animal\"]\n",
    "        self.image_names: List[str] = []\n",
    "\n",
    "        if is_train:\n",
    "            annotations = os.path.join(root, \"annotations\", \"trainval.txt\")\n",
    "        else:\n",
    "            annotations = os.path.join(root, \"annotations\", \"test.txt\")\n",
    "\n",
    "        # Read the annotation file and extract image names\n",
    "        with open(annotations, \"r\") as f:\n",
    "            self.image_names = [image.split(' ')[0] for image in f.readlines()]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, item) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        image_name = self.image_names[item]\n",
    "        image_path = os.path.join(self.root, \"images\", image_name + \".jpg\")\n",
    "        mask_path = os.path.join(self.root, \"annotations\", \"trimaps\", image_name + \".png\")\n",
    "\n",
    "        # Read the image and convert it from BGR to RGB\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # type: ignore\n",
    "\n",
    "        # Read the mask and adjust its values\n",
    "        # 0.299 x Red + 0.587 x Green + 0.114 x Blue\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        mask[mask == 2] = 0 # type: ignore\n",
    "        mask[mask == 3] = 1 # type: ignore\n",
    "\n",
    "        # Apply transformations if provided\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed[\"image\"]\n",
    "            mask = transformed[\"mask\"]\n",
    "\n",
    "        return image, mask # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd482fde",
   "metadata": {},
   "source": [
    "##### Build U-Net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "021feedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DualConv(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        mid_channels: int | None = None,\n",
    "    ):\n",
    "        super(DualConv, self).__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "\n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super(Down, self).__init__()\n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DualConv(in_channels, out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, bilinear: bool = True):\n",
    "        super(Up, self).__init__()\n",
    "\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "            self.conv = DualConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                in_channels // 2,\n",
    "                kernel_size=2,\n",
    "                stride=2,\n",
    "            )\n",
    "            self.conv = DualConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        \"\"\"\n",
    "        x1: from the previous layer - decoder\n",
    "        x2: from the skip connection - encoder\n",
    "        \"\"\"\n",
    "        x1 = self.up(x1)\n",
    "        diffY = x2.size()[2] - x1.size()[2]  # height\n",
    "        diffX = x2.size()[3] - x1.size()[3]  # width\n",
    "\n",
    "        # pad function: (L, R, T, B)\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "\n",
    "        # Concatenate along the channels axis\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNetBaseline(nn.Module):\n",
    "    def __init__(self, in_channels: int, num_classes: int):\n",
    "        super(UNetBaseline, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.inc = DualConv(in_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.down4 = Down(512, 1024)\n",
    "\n",
    "        # Decoder\n",
    "        self.up1 = Up(1024, 512, bilinear=False)\n",
    "        self.up2 = Up(512, 256, bilinear=False)\n",
    "        self.up3 = Up(256, 128, bilinear=False)\n",
    "        self.up4 = Up(128, 64, bilinear=False)\n",
    "\n",
    "        # Output layer\n",
    "        self.outc = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder with skip connections\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)  # Bottleneck\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        x = self.outc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5436d3ed57588a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T13:34:29.722846Z",
     "start_time": "2025-11-26T13:34:29.717420Z"
    }
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class TrainingTorchModel(ABC):\n",
    "    @abstractmethod\n",
    "    def load_model(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def build_model(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def save_model(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def evaluate(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c29eea1",
   "metadata": {},
   "source": [
    "##### Config download checkpoint from Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea8a068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files # type: ignore\n",
    "\n",
    "def download_checkpoint(model_dir: str):\n",
    "    files.download(os.path.join(model_dir, \"best.pth\"))\n",
    "    files.download(os.path.join(model_dir, \"last.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e646db27eb48d8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T13:38:13.747596Z",
     "start_time": "2025-11-26T13:38:13.736982Z"
    }
   },
   "outputs": [],
   "source": [
    "class OxfordIIITPetTraining(TrainingTorchModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        saved_directory: str,\n",
    "        device: str,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model_path = saved_directory\n",
    "        self.device = device\n",
    "\n",
    "        best_model_path = os.path.join(self.model_path, \"best.pth\")\n",
    "        if not os.path.exists(best_model_path):\n",
    "            self.build_model()\n",
    "        else:\n",
    "            self.load_model()\n",
    "\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model is not built or loaded properly.\")\n",
    "\n",
    "    def load_model(self):\n",
    "        checkpoint = torch.load(os.path.join(self.model_path, \"best.pth\"), map_location=self.device)\n",
    "        self.model = self.build_model()\n",
    "        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "    def save_model(self):\n",
    "        print(\"Saving model to: \", self.model_path)\n",
    "\n",
    "    def build_model(self):\n",
    "        self.model = UNetBaseline(in_channels=3, num_classes=1)\n",
    "        self.model = self.model.to(self.device)\n",
    "        return self.model\n",
    "\n",
    "    def evaluate(self):\n",
    "        print(\"Evaluating the model\")\n",
    "\n",
    "    def train(self):\n",
    "        LEARNING_RATE = 0.0001\n",
    "        BATCH_SIZE = 10\n",
    "        EPOCHS = 50\n",
    "        NUM_WORKERS = 4\n",
    "\n",
    "        train_transform = A.Compose([\n",
    "            A.Resize(width=224, height=224),\n",
    "            A.HorizontalFlip(),\n",
    "            A.RandomBrightnessContrast(),\n",
    "            A.Blur(),\n",
    "            A.Sharpen(),\n",
    "            A.RGBShift(),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "        root_dataset = root\n",
    "        train_dataset = OxfordIIIPetDataset(root=root_dataset, is_train=True, transform=train_transform)\n",
    "        val_dataset = OxfordIIIPetDataset(root=root_dataset, is_train=False, transform=train_transform)\n",
    "\n",
    "        train_dataloader = DataLoader(\n",
    "            dataset=train_dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "        )\n",
    "\n",
    "        val_dataloader = DataLoader(\n",
    "            dataset=val_dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            shuffle=False,\n",
    "            drop_last=True,\n",
    "        )\n",
    "\n",
    "        # Initialize optimizer and loss function\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=LEARNING_RATE)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # Metrics\n",
    "        miou_metric = MeanIoU(num_classes=2)\n",
    "        dice_metric = GeneralizedDiceScore(num_classes=2)\n",
    "\n",
    "        # Best validation IoU for saving the best model\n",
    "        best_predict = -1\n",
    "        current_epoch = 0\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(EPOCHS):\n",
    "            # Training Phase\n",
    "            self.model.train()\n",
    "            train_progress = tqdm(train_dataloader, colour=\"cyan\")\n",
    "\n",
    "            for idx, img_mask in enumerate(train_progress):\n",
    "                # B, C, H, W\n",
    "                img = img_mask[0].float().to(self.device)  # type: ignore\n",
    "                # B, H, W\n",
    "                mask = img_mask[1].float().to(self.device)\n",
    "\n",
    "                y_pred = self.model(img)  # B, 1, H, W\n",
    "                y_pred = y_pred.squeeze()  # B, H, W\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Calculate Loss\n",
    "                loss = criterion(y_pred, mask)\n",
    "\n",
    "                # Backpropagation\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_progress.set_description(\"TRAIN| Epoch: {}/{}| Loss: {:0.4f}\".format(epoch, EPOCHS, loss))\n",
    "\n",
    "            # Validation Phase\n",
    "            self.model.eval()\n",
    "\n",
    "            all_losses = []\n",
    "            all_ious = []\n",
    "            all_dices = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for idx, img_mask in enumerate(val_dataloader):\n",
    "                    img = img_mask[0].float().to(self.device)  # type: ignore\n",
    "                    mask = img_mask[1].float().to(self.device)  # B W H\n",
    "\n",
    "                    y_pred = self.model(img)\n",
    "                    y_pred = y_pred.squeeze()  # B H W\n",
    "\n",
    "                    loss = criterion(y_pred, mask)\n",
    "\n",
    "                    mask = mask.long().cpu()\n",
    "                    y_pred[y_pred > 0] = 1  # BWH\n",
    "                    y_pred[y_pred < 0] = 0  # BWH\n",
    "                    y_pred = y_pred.long().cpu()\n",
    "\n",
    "                    miou = miou_metric(y_pred, mask)\n",
    "                    dice = dice_metric(y_pred, mask)\n",
    "\n",
    "                    all_losses.append(loss.cpu().item())\n",
    "                    all_ious.append(miou.cpu().item())\n",
    "                    all_dices.append(dice.cpu().item())\n",
    "\n",
    "                    if idx == 40: break\n",
    "\n",
    "            # Compute mean IoU for the epoch\n",
    "            loss = np.mean(all_losses)\n",
    "            miou = np.mean(all_ious)\n",
    "            dice = np.mean(all_dices)\n",
    "\n",
    "            print(\"VAL| Loss: {:0.4f} | mIOU: {:0.4f} | Dice: {:0.4f}\".format(loss, miou, dice))\n",
    "\n",
    "            checkpoint = {\n",
    "                \"model_state_dict\": self.model.state_dict(),\n",
    "                \"epoch\": epoch,\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"miou\": miou\n",
    "            }\n",
    "\n",
    "            # Save Last Checkpoint\n",
    "            torch.save(checkpoint, os.path.join(self.model_path, \"last.h5\"))\n",
    "\n",
    "            # Save best checkpoint based on IoU\n",
    "            if miou > best_predict:\n",
    "                torch.save(checkpoint, os.path.join(self.model_path, \"best.pth\"))\n",
    "                best_predict = miou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7afc9196bc7c36",
   "metadata": {},
   "source": [
    "##### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46829a02d9d5e171",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T13:38:17.350285Z",
     "start_time": "2025-11-26T13:38:15.696802Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "TRAIN| Epoch: 0/50| Loss: 0.3561: 100%|\u001b[36m██████████\u001b[0m| 368/368 [02:36<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL| Loss: 0.4035 | mIOU: 0.5976 | Dice: 0.5375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN| Epoch: 1/50| Loss: 0.2891: 100%|\u001b[36m██████████\u001b[0m| 368/368 [02:36<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL| Loss: 0.4115 | mIOU: 0.5706 | Dice: 0.5574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN| Epoch: 2/50| Loss: 0.3705: 100%|\u001b[36m██████████\u001b[0m| 368/368 [02:36<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL| Loss: 0.3251 | mIOU: 0.6717 | Dice: 0.5910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN| Epoch: 3/50| Loss: 0.3481: 100%|\u001b[36m██████████\u001b[0m| 368/368 [02:36<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL| Loss: 0.2988 | mIOU: 0.6928 | Dice: 0.6160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN| Epoch: 4/50| Loss: 0.2364: 100%|\u001b[36m██████████\u001b[0m| 368/368 [02:36<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL| Loss: 0.2939 | mIOU: 0.6958 | Dice: 0.6391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN| Epoch: 5/50| Loss: 0.2441: 100%|\u001b[36m██████████\u001b[0m| 368/368 [02:36<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL| Loss: 0.2792 | mIOU: 0.7082 | Dice: 0.6531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN| Epoch: 6/50| Loss: 0.2691: 100%|\u001b[36m██████████\u001b[0m| 368/368 [02:36<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL| Loss: 0.2726 | mIOU: 0.7092 | Dice: 0.6593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN| Epoch: 7/50| Loss: 0.1705: 100%|\u001b[36m██████████\u001b[0m| 368/368 [02:36<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL| Loss: 0.2850 | mIOU: 0.7028 | Dice: 0.6604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN| Epoch: 8/50| Loss: 0.2621: 100%|\u001b[36m██████████\u001b[0m| 368/368 [02:36<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL| Loss: 0.2664 | mIOU: 0.7290 | Dice: 0.6627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN| Epoch: 9/50| Loss: 0.2213: 100%|\u001b[36m██████████\u001b[0m| 368/368 [02:36<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL| Loss: 0.2563 | mIOU: 0.7370 | Dice: 0.6615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN| Epoch: 10/50| Loss: 0.2049: 100%|\u001b[36m██████████\u001b[0m| 368/368 [02:36<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL| Loss: 0.2523 | mIOU: 0.7316 | Dice: 0.6841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN| Epoch: 11/50| Loss: 0.3066: 100%|\u001b[36m██████████\u001b[0m| 368/368 [02:36<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL| Loss: 0.2963 | mIOU: 0.6881 | Dice: 0.6595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN| Epoch: 12/50| Loss: 0.1748: 100%|\u001b[36m██████████\u001b[0m| 368/368 [02:36<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL| Loss: 0.2340 | mIOU: 0.7463 | Dice: 0.6922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN| Epoch: 13/50| Loss: 0.1879: 100%|\u001b[36m██████████\u001b[0m| 368/368 [02:36<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL| Loss: 0.2172 | mIOU: 0.7559 | Dice: 0.7074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN| Epoch: 14/50| Loss: 0.1926: 100%|\u001b[36m██████████\u001b[0m| 368/368 [02:36<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL| Loss: 0.2556 | mIOU: 0.7306 | Dice: 0.6867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN| Epoch: 15/50| Loss: 0.1764: 100%|\u001b[36m██████████\u001b[0m| 368/368 [02:36<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL| Loss: 0.2350 | mIOU: 0.7623 | Dice: 0.6889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN| Epoch: 16/50| Loss: 0.1324: 100%|\u001b[36m██████████\u001b[0m| 368/368 [02:36<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL| Loss: 0.2038 | mIOU: 0.7717 | Dice: 0.7012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN| Epoch: 17/50| Loss: 0.2128: 100%|\u001b[36m██████████\u001b[0m| 368/368 [02:36<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL| Loss: 0.2001 | mIOU: 0.7773 | Dice: 0.7122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN| Epoch: 18/50| Loss: 0.1641: 100%|\u001b[36m██████████\u001b[0m| 368/368 [02:36<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL| Loss: 0.2040 | mIOU: 0.7768 | Dice: 0.7104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN| Epoch: 19/50| Loss: 0.1301: 100%|\u001b[36m██████████\u001b[0m| 368/368 [02:36<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL| Loss: 0.2068 | mIOU: 0.7782 | Dice: 0.7043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN| Epoch: 20/50| Loss: 0.1271: 100%|\u001b[36m██████████\u001b[0m| 368/368 [02:36<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL| Loss: 0.1950 | mIOU: 0.7791 | Dice: 0.7172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN| Epoch: 21/50| Loss: 0.1496:  27%|\u001b[36m██▋       \u001b[0m| 99/368 [00:42<01:53,  2.36it/s]"
     ]
    }
   ],
   "source": [
    "device = torch.accelerator.current_accelerator() or \"cpu\"\n",
    "saved_directory = \"/content/saved\"\n",
    "os.makedirs(saved_directory, exist_ok=True)\n",
    "\n",
    "entire = OxfordIIITPetTraining(saved_directory=saved_directory, device=device)\n",
    "entire.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ca1384",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel 'Python 3 (ipykernel) (traning_oxford_iiit_pet.ipynb)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. Unable to get resolved server information for google.colab:colab:ddd350ca-2b45-4f98-8fce-7c6159246b04"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
